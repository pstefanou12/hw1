{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FU7xWiY6TyWS"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "!(stat -t /usr/local/lib/*/dist-packages/google/colab > /dev/null 2>&1) && exit \n",
    "rm -rf hw1\n",
    "git clone https://github.com/mit-6864/hw1.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "A0MHaHrdUACZ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:30: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:167: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:284: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_Gram=True, verbose=0,\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:862: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1101: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1127: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, positive=False):\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1362: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1602: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1738: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, positive=False):\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/decomposition/online_lda.py:29: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  EPS = np.finfo(np.float).eps\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/content/hw1\")\n",
    "sys.path.append('/opt/anaconda3/lib/python3.7/site-packages/')\n",
    "\n",
    "import csv\n",
    "import itertools as it\n",
    "import numpy as np\n",
    "import sklearn.decomposition\n",
    "np.random.seed(0)\n",
    "from tqdm import tqdm\n",
    "\n",
    "import lab_util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cZ3MUj4iUf76"
   },
   "source": [
    "# Introduction\n",
    "\n",
    "In this notebook, you will find code scaffolding for the word representation parts of Homework 1 (matrix factorization and Word2Vec-style language modeling; code for the HMM section of the assignment is released in another notebook). There are certain parts of the scaffolding marked with `# Your code here!` comments where you can fill in code to perform the specified tasks. After implementing the methods in this notebook, you will need to design and perform experiments to evaluate each method and respond to the questions in the Homework 1 handout (available on Canvas). You should be able to complete this assignment without changing any of the scaffolding code, just writing code to fill in the scaffolding and run experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gG654Y9J3yHw"
   },
   "source": [
    "## Dataset\n",
    "\n",
    "We're going to be working with a dataset of product reviews. The following cell loads the dataset and splits it into training, validation, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "JwiX-Tc9V1xI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review: I have bought several of the Vitality canned dog food products and have found them all to be of good quality. The product looks more like a stew than a processed meat and it smells better. My Labrador is finicky and she appreciates this product better than  most.\n",
      "rating: 1 (good)\n",
      "\n",
      "review: Product arrived labeled as Jumbo Salted Peanuts...the peanuts were actually small sized unsalted. Not sure if this was an error or if the vendor intended to represent the product as \"Jumbo\".\n",
      "rating: 0 (bad)\n",
      "\n",
      "review: This is a confection that has been around a few centuries.  It is a light, pillowy citrus gelatin with nuts - in this case Filberts. And it is cut into tiny squares and then liberally coated with powdered sugar.  And it is a tiny mouthful of heaven.  Not too chewy, and very flavorful.  I highly recommend this yummy treat.  If you are familiar with the story of C.S. Lewis' \"The Lion, The Witch, and The Wardrobe\" - this is the treat that seduces Edmund into selling out his Brother and Sisters to the Witch.\n",
      "rating: 1 (good)\n",
      "\n",
      "review: If you are looking for the secret ingredient in Robitussin I believe I have found it.  I got this in addition to the Root Beer Extract I ordered (which was good) and made some cherry soda.  The flavor is very medicinal.\n",
      "rating: 0 (bad)\n",
      "\n",
      "review: Great taffy at a great price.  There was a wide assortment of yummy taffy.  Delivery was very quick.  If your a taffy lover, this is a deal.\n",
      "rating: 1 (good)\n",
      "\n",
      "review: I got a wild hair for taffy and ordered this five pound bag. The taffy was all very enjoyable with many flavors: watermelon, root beer, melon, peppermint, grape, etc. My only complaint is there was a bit too much red/black licorice-flavored pieces (just not my particular favorites). Between me, my kids, and my husband, this lasted only two weeks! I would recommend this brand of taffy -- it was a delightful treat.\n",
      "rating: 1 (good)\n",
      "\n",
      "Read 4000 total reviews.\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "n_positive = 0\n",
    "n_disp = 0\n",
    "with open(\"/Users/patroklos/Desktop/6.864/hw1/reviews.csv\") as reader:\n",
    "  csvreader = csv.reader(reader)\n",
    "  next(csvreader)\n",
    "  for id, review, label in csvreader:\n",
    "    label = int(label)\n",
    "\n",
    "    # hacky class balancing\n",
    "    if label == 1:\n",
    "      if n_positive == 2000:\n",
    "        continue\n",
    "      n_positive += 1\n",
    "    if len(data) == 4000:\n",
    "      break\n",
    "\n",
    "    data.append((review, label))\n",
    "    \n",
    "    if n_disp > 5:\n",
    "      continue\n",
    "    n_disp += 1\n",
    "    print(\"review:\", review)\n",
    "    print(\"rating:\", label, \"(good)\" if label == 1 else \"(bad)\")\n",
    "    print()\n",
    "\n",
    "print(f\"Read {len(data)} total reviews.\")\n",
    "np.random.shuffle(data)\n",
    "reviews, labels = zip(*data)\n",
    "train_reviews = reviews[:3000]\n",
    "train_labels = labels[:3000]\n",
    "val_reviews = reviews[3000:3500]\n",
    "val_labels = labels[3000:3500]\n",
    "test_reviews = reviews[3500:]\n",
    "test_labels = labels[3500:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "twLHWqM6Z5xD"
   },
   "source": [
    "# Part 1: word representations via matrix factorization\n",
    "\n",
    "First, we'll construct the term-document matrix (look at `/content/hw1/lab_util.py` in the file browser on the left if you want to see how this works)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "3WPt6Y7-Z_7P"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TD matrix is 2006 x 3000\n"
     ]
    }
   ],
   "source": [
    "vectorizer = lab_util.CountVectorizer()\n",
    "vectorizer.fit(train_reviews)\n",
    "td_matrix = vectorizer.transform(train_reviews).T\n",
    "print(f\"TD matrix is {td_matrix.shape[0]} x {td_matrix.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hd3-pw4XbD4B"
   },
   "source": [
    "First, implement the function `learn_reps_lsa` that computes word representations via latent semantic analysis. The `sklearn.decomposition` or `np.linalg` packages may be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "KASVs8KubeBE"
   },
   "outputs": [],
   "source": [
    "import sklearn.decomposition\n",
    "def learn_reps_lsa(matrix, rep_size):\n",
    "    # `matrix` is a `|V| x n` matrix, where `|V|` is the number of words in the\n",
    "    # vocabulary. This function should return a `|V| x rep_size` matrix with each\n",
    "    # row corresponding to a word representation.\n",
    "\n",
    "    svd = sklearn.decomposition.TruncatedSVD(n_components=rep_size)\n",
    "    return svd.fit_transform(matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0fxv02wO9LbS"
   },
   "source": [
    "#### Sanity check 1\n",
    "The following cell contains a simple sanity check for your `learn_reps_lsa` implementation: it should print `True` if your `learn_reps_lsa` function is implemented equivalently to one of our solutions.  There are at least two reasonable ways to formulate these LSA word representations (whether you directly use the left singular vectors of `matrix` or scale them by the singular values), these correspond to the two possible representations in the sanity check below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "KsRlKB9q9Js-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "DEBUG_sc1_matrix = np.array([[1,0,0,2,1,3,5],\n",
    "                             [2,0,0,0,0,4,0],\n",
    "                             [0,3,4,1,8,6,6],\n",
    "                             [1,4,5,0,0,0,0]])\n",
    "\n",
    "other = DEBUG_sc1_matrix.dot(DEBUG_sc1_matrix.T)\n",
    "other_reps = learn_reps_lsa(other, 3)\n",
    "\n",
    "DEBUG_reps = learn_reps_lsa(DEBUG_sc1_matrix, 3)\n",
    "\n",
    "DEBUG_gt1 = np.array([[ -4.92017554,  -2.85465774,   1.18575453],\n",
    "                      [ -2.14977584,  -1.19987977,   3.37221899],\n",
    "                      [-12.62664695,   0.10890093,  -1.32131745],\n",
    "                      [ -2.69216011,   5.66453534,   1.33728063]])\n",
    "DEBUG_gt2 = np.array([[-0.35188159, -0.44213061,  0.29358929],\n",
    "                      [-0.15374788, -0.18583789,  0.83495136],\n",
    "                      [-0.90303377,  0.01686662, -0.32715426],\n",
    "                      [-0.19253817,  0.87732566,  0.3311067 ]])\n",
    "\n",
    "print(np.allclose(np.abs(DEBUG_reps), np.abs(DEBUG_gt1)) or np.allclose(np.abs(DEBUG_reps), np.abs(DEBUG_gt2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SKWzRC0dclVK"
   },
   "source": [
    "Let's look at some representations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "-Ad7RZkwceWw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good 47\n",
      "  . 1.056\n",
      "  a 1.101\n",
      "  but 1.121\n",
      "  , 1.152\n",
      "  the 1.157\n",
      "bad 201\n",
      "  . 1.395\n",
      "  taste 1.416\n",
      "  but 1.434\n",
      "  a 1.434\n",
      "  i 1.449\n",
      "cookie 504\n",
      "  nana's 0.789\n",
      "  cookies 1.047\n",
      "  oreos 1.308\n",
      "  bars 1.355\n",
      "  bites 1.413\n",
      "jelly 351\n",
      "  twist 1.092\n",
      "  cardboard 1.243\n",
      "  peanuts 1.402\n",
      "  advertised 1.412\n",
      "  plastic 1.436\n",
      "dog 925\n",
      "  food 1.048\n",
      "  pet 1.065\n",
      "  pets 1.066\n",
      "  switched 1.203\n",
      "  foods 1.230\n",
      "the 36\n",
      "  . 0.331\n",
      "  <unk> 0.366\n",
      "  of 0.395\n",
      "  and 0.403\n",
      "  to 0.422\n",
      "3 289\n",
      "  8 1.212\n",
      "  . 1.239\n",
      "  the 1.271\n",
      "  to 1.273\n",
      "  <unk> 1.279\n"
     ]
    }
   ],
   "source": [
    "reps = learn_reps_lsa(td_matrix, 500)\n",
    "words = [\"good\", \"bad\", \"cookie\", \"jelly\", \"dog\", \"the\", \"3\"]\n",
    "show_tokens = [vectorizer.tokenizer.word_to_token[word] for word in words]\n",
    "lab_util.show_similar_words(vectorizer.tokenizer, reps, show_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LsOAGLB3iRjT"
   },
   "source": [
    "We've been operating on the raw count matrix, but in class we discussed several reweighting schemes aimed at making LSA representations more informative. \n",
    "\n",
    "Here, implement the TF-IDF transform and see how it affects learned representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "1y3PmW-IgpqA"
   },
   "outputs": [],
   "source": [
    "def transform_tfidf(matrix):\n",
    "    # `matrix` is a `|V| x |D|` matrix of raw counts, where `|V|` is the \n",
    "    # vocabulary size and `|D|` is the number of documents in the corpus. This\n",
    "    # function should (nondestructively) return a version of `matrix` with the\n",
    "    # TF-IDF transform applied.\n",
    "\n",
    "    mask = np.where(matrix > 0, 1, 0)\n",
    "    return matrix * np.log(matrix.shape[1] / np.expand_dims(mask.sum(1), 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CnfIOUYjEZjw"
   },
   "source": [
    "#### Sanity check 2\n",
    "The following cell should print `True` if your `transform_tfidf` function is implemented properly. (*Hint: in our implementation, we use the natural logarithm (base $e$) when computing inverse document frequency.*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "KVtphNeDEj2W"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "DEBUG_sc2_matrix = np.array([[3,1,0,3,0],\n",
    "                             [0,2,0,0,1],\n",
    "                             [7,8,2,0,1],\n",
    "                             [1,9,8,1,0]])\n",
    "DEBUG_gt = np.array([[1.53247687, 0.51082562, 0.        , 1.53247687, 0.        ],\n",
    "                     [0.        , 1.83258146, 0.        , 0.        , 0.91629073],\n",
    "                     [1.56200486, 1.78514841, 0.4462871 , 0.        , 0.22314355],\n",
    "                     [0.22314355, 2.00829196, 1.78514841, 0.22314355, 0.        ]])\n",
    "print(np.allclose(transform_tfidf(DEBUG_sc2_matrix), DEBUG_gt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xOprgqzHi7bk"
   },
   "source": [
    "How does this change the learned similarity function?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "SV5xKLYTi7LA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good 47\n",
      "  but 0.429\n",
      "  . 0.553\n",
      "  like 0.608\n",
      "  not 0.608\n",
      "  as 0.613\n",
      "bad 201\n",
      "  taste 0.575\n",
      "  like 0.615\n",
      "  but 0.705\n",
      "  just 0.743\n",
      "  . 0.780\n",
      "cookie 504\n",
      "  cookies 0.379\n",
      "  nana's 0.440\n",
      "  oreos 0.792\n",
      "  bars 0.863\n",
      "  shortbread 0.933\n",
      "jelly 351\n",
      "  gifts 0.817\n",
      "  creamer 1.013\n",
      "  packages 1.034\n",
      "  advertised 1.093\n",
      "  twist 1.109\n",
      "dog 925\n",
      "  pet 0.722\n",
      "  foods 0.762\n",
      "  switched 0.777\n",
      "  pets 0.806\n",
      "  food 0.851\n",
      "the 36\n",
      "  . 0.118\n",
      "  of 0.155\n",
      "  and 0.158\n",
      "  to 0.160\n",
      "  <unk> 0.209\n",
      "3 289\n",
      "  8 0.770\n",
      "  1 0.784\n",
      "  2 0.812\n",
      "  per 0.868\n",
      "  to 0.878\n"
     ]
    }
   ],
   "source": [
    "td_matrix_tfidf = transform_tfidf(td_matrix)\n",
    "# reps_tfidf = learn_reps_lsa(td_matrix_tfidf, 500)\n",
    "reps_tfidf = learn_reps_lsa(td_matrix_tfidf, 100)\n",
    "lab_util.show_similar_words(vectorizer.tokenizer, reps_tfidf, show_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HO-NG4u1kG9z"
   },
   "source": [
    "Now that we have some representations, let's see if we can do something useful with them.\n",
    "\n",
    "Below, implement a feature function that represents a document as the sum of its\n",
    "learned word embeddings.\n",
    "\n",
    "The remaining code trains a logistic regression model on a set of *labeled* reviews; we're interested in seeing how much representations learned from *unlabeled* reviews improve classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "6B08xvIFlee3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word features, 3000 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/base.py:291: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  indices = (scores > 0).astype(np.int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.784 \n",
      "\n",
      "lsa features, 3000 examples\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for ** or pow(): 'NoneType' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-b74ced4caacb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0mn_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0mtraining_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_featurizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0mtraining_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"lsa\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlsa_featurizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0mtraining_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"combo\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcombo_featurizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-b74ced4caacb>\u001b[0m in \u001b[0;36mtraining_experiment\u001b[0;34m(name, featurizer, n_train)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mtest_xs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_reviews\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mtest_ys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeaturizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_xs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_ys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeaturizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_xs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_ys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-b74ced4caacb>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(featurizer, xs, ys)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeaturizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mxs_featurized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeaturizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs_featurized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-b74ced4caacb>\u001b[0m in \u001b[0;36mlsa_featurizer\u001b[0;34m(xs)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# normalize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfeats\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeats\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# We've implemented the remainder of the training and evaluation pipeline,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for ** or pow(): 'NoneType' and 'int'"
     ]
    }
   ],
   "source": [
    "import sklearn.linear_model\n",
    "\n",
    "def word_featurizer(xs):\n",
    "    # normalize\n",
    "    return xs / np.sqrt((xs ** 2).sum(axis=1, keepdims=True))\n",
    "\n",
    "def lsa_featurizer(xs):\n",
    "    # This function takes in a matrix in which each row contains the word counts\n",
    "    # for the given review. It should return a matrix in which each row contains\n",
    "    # the learned feature representation of each review (e.g. the sum of LSA \n",
    "    # word representations).\n",
    "\n",
    "    feats = None  # Your code here!\n",
    "\n",
    "    # normalize\n",
    "    return feats / np.sqrt((feats ** 2).sum(axis=1, keepdims=True))\n",
    "\n",
    "# We've implemented the remainder of the training and evaluation pipeline,\n",
    "# so you likely won't need to modify the following four functions.\n",
    "def combo_featurizer(xs):\n",
    "    return np.concatenate((word_featurizer(xs), lsa_featurizer(xs)), axis=1)\n",
    "\n",
    "def train_model(featurizer, xs, ys):\n",
    "    xs_featurized = featurizer(xs)\n",
    "    model = sklearn.linear_model.LogisticRegression()\n",
    "    model.fit(xs_featurized, ys)\n",
    "    return model\n",
    "\n",
    "def eval_model(model, featurizer, xs, ys):\n",
    "    xs_featurized = featurizer(xs)\n",
    "    pred_ys = model.predict(xs_featurized)\n",
    "    return np.mean(pred_ys == ys)\n",
    "\n",
    "def training_experiment(name, featurizer, n_train):\n",
    "    print(f\"{name} features, {n_train} examples\")\n",
    "    train_xs = vectorizer.transform(train_reviews[:n_train])\n",
    "    train_ys = train_labels[:n_train]\n",
    "    test_xs = vectorizer.transform(test_reviews)\n",
    "    test_ys = test_labels\n",
    "    model = train_model(featurizer, train_xs, train_ys)\n",
    "    acc = eval_model(model, featurizer, test_xs, test_ys)\n",
    "    print(acc, '\\n')\n",
    "    return acc\n",
    "\n",
    "# The following four lines will run a training experiment with all 3k examples\n",
    "# in training set for each feature type. `training_experiment` may be useful to\n",
    "# you when performing experiments to answer questions in Part 1 of the Homework\n",
    "# 1 handout.\n",
    "n_train = 3000\n",
    "training_experiment(\"word\", word_featurizer, n_train)\n",
    "training_experiment(\"lsa\", lsa_featurizer, n_train)\n",
    "training_experiment(\"combo\", combo_featurizer, n_train)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rpXziVNrlfp2"
   },
   "source": [
    "**Part 1: Lab writeup**\n",
    "\n",
    "Part 1 of your lab report should discuss any implementation details that were important to filling out the code above, as well as your answers to the questions in Part 1 of the Homework 1 handout. Below, you can set up and perform experiments that answer these questions (include figures, plots, and tables in your write-up as you see fit)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L3WrzsHhC1I5"
   },
   "source": [
    "## Experiments for Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aTVF9JdRNOsx"
   },
   "outputs": [],
   "source": [
    "# Your code here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AxfunCYh5nmZ"
   },
   "source": [
    "## Part 2: word representations via language modeling\n",
    "\n",
    "In this section, we'll train a word embedding model with a word2vec-style objective rather than a matrix factorization objective. This requires a little more work; we've provided scaffolding for a PyTorch model implementation below.\n",
    "If you don't have much PyTorch experience, there are some tutorials [here](https://pytorch.org/tutorials/) which may be useful. You're also welcome to implement these experiments in any other framework of your choosing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M1napibQ6aub"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as torch_data\n",
    "\n",
    "class Word2VecModel(nn.Module):\n",
    "    # A torch module implementing a word2vec predictor. The `forward` function\n",
    "    # should take a batch of context word ids as input and predict the word \n",
    "    # in the middle of the context as output, as in the CBOW model from lecture.\n",
    "\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        # Your code here!\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def forward(self, context):\n",
    "        # Context is an `n_batch x n_context` matrix of integer word ids\n",
    "        # this function should return a set of scores for predicting the word \n",
    "        # in the middle of the context\n",
    "\n",
    "        # Your code here!\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ePgZlityuWr3"
   },
   "outputs": [],
   "source": [
    "def learn_reps_word2vec(corpus, window_size, rep_size, n_epochs, n_batch):\n",
    "    #This method takes in a corpus of training sentences. It returns a matrix of\n",
    "    # word embeddings with the same structure as used in the previous section of \n",
    "    # the assignment. (You can extract this matrix from the parameters of the \n",
    "    # Word2VecModel.)\n",
    "\n",
    "    tokenizer = lab_util.Tokenizer()\n",
    "    tokenizer.fit(corpus)\n",
    "    tokenized_corpus = tokenizer.tokenize(corpus)\n",
    "\n",
    "    ngrams = lab_util.get_ngrams(tokenized_corpus, window_size)\n",
    "\n",
    "    device = torch.device('cuda')  # run on colab gpu\n",
    "    model = Word2VecModel(tokenizer.vocab_size, rep_size).to(device)\n",
    "    opt = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    loader = torch_data.DataLoader(ngrams, batch_size=n_batch, shuffle=True)\n",
    "\n",
    "    # What loss function should we use for Word2Vec?\n",
    "    loss_fn = None  # Your code here!\n",
    "\n",
    "    losses = []  # Potentially useful for debugging (loss should go down!)\n",
    "    for epoch in tqdm(range(n_epochs)):\n",
    "        epoch_loss = 0\n",
    "        for context, label in loader:\n",
    "            # As described above, `context` is a batch of context word ids, and\n",
    "            # `label` is a batch of predicted word labels.\n",
    "\n",
    "            # Here, perform a forward pass to compute predictions for the model.\n",
    "            preds = None  # Your code here!\n",
    "\n",
    "\n",
    "            # Now finish the backward pass and gradient update.\n",
    "            # Remember, you need to compute the loss, zero the gradients\n",
    "            # of the model parameters, perform the backward pass, and\n",
    "            # update the model parameters.\n",
    "            loss = None  # Your code here!\n",
    "\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "        losses.append(epoch_loss)\n",
    "\n",
    "    # Hint: you want to return a `vocab_size x embedding_size` numpy array\n",
    "    embedding_matrix = None  # Your code here!\n",
    "\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aaUy1cNuB3W1"
   },
   "outputs": [],
   "source": [
    "# Use the function you just wrote to learn Word2Vec embeddings:\n",
    "reps_word2vec = learn_reps_word2vec(train_reviews, 2, 500, 10, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O3oE-tpR7I39"
   },
   "source": [
    "After training the embeddings, we can try to visualize the embedding space to see if it makes sense. First, we can take any word in the space and check its closest neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yMW4QND56bHF"
   },
   "outputs": [],
   "source": [
    "lab_util.show_similar_words(vectorizer.tokenizer, reps_word2vec, show_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ue-9CPSc7fi9"
   },
   "source": [
    "We can also cluster the embedding space. Clustering in 4 or more dimensions is hard to visualize, and even clustering in 2 or 3 can be difficult because there are so many words in the vocabulary. One thing we can try to do is assign cluster labels and qualitiatively look for an underlying pattern in the clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v-Yf6NMCXVx4"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "indices = KMeans(n_clusters=10).fit_predict(reps_word2vec)\n",
    "zipped = list(zip(range(vectorizer.tokenizer.vocab_size), indices))\n",
    "np.random.shuffle(zipped)\n",
    "zipped = zipped[:100]\n",
    "zipped = sorted(zipped, key=lambda x: x[1])\n",
    "for token, cluster_idx in zipped:\n",
    "    word = vectorizer.tokenizer.token_to_word[token]\n",
    "    print(f\"{word}: {cluster_idx}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ci1TkENU78Wn"
   },
   "source": [
    "Finally, we can use the trained word embeddings to construct vector representations of full reviews. One common approach is to simply average all the word embeddings in the review to create an overall embedding. Implement the transform function in Word2VecFeaturizer to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A5vjmRV6Dgbu"
   },
   "outputs": [],
   "source": [
    "def w2v_featurizer(xs):\n",
    "    # This function takes in a matrix in which each row contains the word counts\n",
    "    # for the given review. It should return a matrix in which each row contains\n",
    "    # the average Word2Vec embedding of each review (hint: this will be very\n",
    "    # similar to `lsa_featurizer` from above, just using Word2Vec embeddings \n",
    "    # instead of LSA).\n",
    "\n",
    "    feats = None # Your code here!\n",
    "\n",
    "    # normalize\n",
    "    return feats / np.sqrt((feats ** 2).sum(axis=1, keepdims=True))\n",
    "\n",
    "training_experiment(\"word2vec\", w2v_featurizer, 3000)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XSfoQbxaXtfH"
   },
   "source": [
    "**Part 2: Lab writeup**\n",
    "\n",
    "Part 2 of your lab report should discuss any implementation details that were important to filling out the code above, as well as your answers to the questions in Part 2 of the Homework 1 handout. Below, you can set up and perform experiments that answer these questions (include figures, plots, and tables in your write-up as you see fit)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Beb7dgqWycYx"
   },
   "source": [
    "## Experiments for Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "obr-zEmNydDm"
   },
   "outputs": [],
   "source": [
    "# Your code here!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "6864_hw1_part_1-2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
